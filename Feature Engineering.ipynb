{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a98f0bd-e40e-4845-9a08-bbb282f4837b",
   "metadata": {},
   "source": [
    "**Q.1) What is a parameter?**\n",
    "\n",
    "**Ans:-**\n",
    "In Machine Learning, a **parameter** is a configuration variable that is learned from the training data by the model. Parameters define the internal structure of the model and influence how the model makes predictions. For example, in a linear regression model, the coefficients (weights) and the intercept (bias) are parameters. These values are adjusted during the training process to minimize the error between predicted and actual results. Parameters are not set manually—they are automatically learned through optimization algorithms like Gradient Descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f453227-6210-44b0-9eaa-5929a96bed1a",
   "metadata": {},
   "source": [
    "**Q.2) What is correlation?**\n",
    "\n",
    "**Ans:**\n",
    "**Correlation** is a statistical measure that describes the strength and direction of the relationship between two variables. It helps in understanding whether an increase or decrease in one variable corresponds to an increase or decrease in another. Correlation values range from **-1 to +1**:\n",
    "\n",
    "* A value of **+1** indicates a perfect positive correlation (both variables increase together).\n",
    "* A value of **-1** indicates a perfect negative correlation (one increases while the other decreases).\n",
    "* A value of **0** means no correlation (variables do not affect each other).\n",
    "\n",
    "In Machine Learning, correlation is useful in feature selection, helping to identify which variables are related and which may be redundant.\n",
    "\n",
    "\n",
    "**Negative correlation** means that as one variable increases, the other variable tends to decrease. It indicates an **inverse relationship** between two variables. The correlation coefficient for a negative correlation lies between **0 and -1**.\n",
    "\n",
    "For example, if the number of hours a person exercises increases, their weight might decrease — showing a negative correlation. In Machine Learning, identifying negative correlations helps in understanding the impact of one feature on another and in selecting the most relevant variables for model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daf9c95-15f7-4684-af0c-e9fbb596d6e4",
   "metadata": {},
   "source": [
    "**Q.3) Define Machine Learning. What are the main components in Machine Learning?**\n",
    "\n",
    "**Ans:**\n",
    "**Machine Learning (ML)** is a branch of artificial intelligence that enables systems to automatically learn and improve from experience without being explicitly programmed. It focuses on building models that can analyze data, identify patterns, and make decisions or predictions based on the input data.\n",
    "\n",
    "The **main components** of Machine Learning are:\n",
    "\n",
    "1. **Data** – Raw input that the model learns from.\n",
    "2. **Model** – A mathematical structure that makes predictions or decisions based on data.\n",
    "3. **Training** – The process where the model learns patterns from the data.\n",
    "4. **Evaluation** – Measuring the model's accuracy using metrics like accuracy, precision, recall, or loss.\n",
    "5. **Prediction** – Using the trained model to make forecasts on new or unseen data.\n",
    "\n",
    "These components work together to create intelligent systems capable of solving real-world problems such as classification, regression, and clustering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfb0135-55ae-4104-8678-08b6c38abd52",
   "metadata": {},
   "source": [
    "**Q.4) How does loss value help in determining whether the model is good or not?**\n",
    "\n",
    "**Ans:**\n",
    "The **loss value** is a numerical measure that indicates how well or poorly a Machine Learning model is performing. It calculates the difference between the model’s predicted output and the actual target values. A **lower loss** value means the model's predictions are close to the actual values, suggesting better performance. Conversely, a **high loss** value indicates the model is making inaccurate predictions.\n",
    "\n",
    "During training, optimization algorithms (like Gradient Descent) try to minimize the loss function by adjusting the model’s parameters. Monitoring the loss value helps determine whether the model is learning effectively or if it is underfitting or overfitting. Thus, the **loss value acts as a key indicator of model quality**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7f7ccb-8ccf-413f-88af-6327e67a5a07",
   "metadata": {},
   "source": [
    "**Q.5) What are continuous and categorical variables?**\n",
    "\n",
    "**Ans:**\n",
    "In Machine Learning, variables (features) are classified into two main types: **continuous** and **categorical**.\n",
    "\n",
    "* **Continuous Variables:**\n",
    "  These are numeric variables that can take an infinite number of values within a range. They are measurable and can have decimal points.\n",
    "  *Examples:* Age, height, weight, temperature, salary.\n",
    "\n",
    "* **Categorical Variables:**\n",
    "  These are variables that represent categories or groups. They contain a limited number of distinct values and are often non-numeric.\n",
    "  *Examples:* Gender (Male/Female), Marital Status (Single/Married), City names, Blood Type.\n",
    "\n",
    "Understanding the type of variable is important because **different preprocessing techniques** are used for continuous and categorical variables in Machine Learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e07ab75-2116-4e90-a15f-904bec888d9f",
   "metadata": {},
   "source": [
    "**Q.6) How do we handle categorical variables in Machine Learning? What are the common techniques?**\n",
    "\n",
    "**Ans:**\n",
    "Categorical variables must be converted into numerical form before they can be used in Machine Learning models, as most algorithms work only with numbers.\n",
    "\n",
    "**Common techniques to handle categorical variables include:**\n",
    "\n",
    "1. **Label Encoding:**\n",
    "   Assigns a unique integer to each category. Useful for ordinal data (e.g., Low=0, Medium=1, High=2).\n",
    "   *Tool:* `LabelEncoder` from `sklearn.preprocessing`.\n",
    "\n",
    "2. **One-Hot Encoding:**\n",
    "   Creates separate binary columns for each category (1 for present, 0 for others). Best for nominal (non-ordered) data.\n",
    "   *Tool:* `OneHotEncoder` or `pd.get_dummies()` in pandas.\n",
    "\n",
    "3. **Ordinal Encoding:**\n",
    "   Similar to label encoding but preserves category order (used for ordered categories).\n",
    "   *Example:* Education level – High School < Graduate < Postgraduate.\n",
    "\n",
    "These encoding methods help models interpret categorical data effectively, improving accuracy and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b389ae7a-1ebb-4709-8adf-0ec486c1a9dc",
   "metadata": {},
   "source": [
    "**Q.7) What do you mean by training and testing a dataset?**\n",
    "\n",
    "**Ans:**\n",
    "In Machine Learning, the dataset is typically divided into two parts: **training set** and **testing set**.\n",
    "\n",
    "* **Training Dataset:**\n",
    "  This is the portion of the data used to train the model. The model learns the relationships, patterns, and rules from this data by adjusting its internal parameters.\n",
    "\n",
    "* **Testing Dataset:**\n",
    "  This is the separate portion of data used to evaluate the model's performance. It checks how well the model generalizes to new, unseen data.\n",
    "\n",
    "Splitting the dataset helps avoid **overfitting**, where the model performs well on training data but poorly on new data. A common split ratio is **80% training and 20% testing** or **70-30**, depending on the dataset size.\n",
    "\n",
    "In Python, this can be done using:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)  \n",
    "```\n",
    "\n",
    "This approach ensures the model is both accurate and reliable in real-world scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dabd20-39f8-43df-b252-df698163584f",
   "metadata": {},
   "source": [
    "**Q.8) What is sklearn.preprocessing?**\n",
    "\n",
    "**Ans:**\n",
    "`sklearn.preprocessing` is a module in the **Scikit-learn** library that provides various tools and functions for preparing data before training a Machine Learning model. Preprocessing ensures that the data is in the right format and scale for better model performance and accuracy.\n",
    "\n",
    "**Key functionalities of `sklearn.preprocessing` include:**\n",
    "\n",
    "1. **Scaling features:**\n",
    "\n",
    "   * `StandardScaler`: Scales data to have zero mean and unit variance.\n",
    "   * `MinMaxScaler`: Scales data to a fixed range, typically \\[0, 1].\n",
    "\n",
    "2. **Encoding categorical variables:**\n",
    "\n",
    "   * `LabelEncoder`: Converts categorical labels into numeric values.\n",
    "   * `OneHotEncoder`: Converts categories into binary columns (one-hot vectors).\n",
    "\n",
    "3. **Imputation:**\n",
    "\n",
    "   * `SimpleImputer`: Handles missing values by filling them with mean, median, or mode.\n",
    "\n",
    "4. **Normalization:**\n",
    "\n",
    "   * `Normalizer`: Scales input vectors individually to unit norm (mostly for text or sparse data).\n",
    "\n",
    "Using `sklearn.preprocessing` helps standardize the data, making it easier for ML algorithms to learn efficiently and produce accurate results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dd2b19-0836-4dd1-82a8-780c0e5b0915",
   "metadata": {},
   "source": [
    "**Q.9) What is a Test set?**\n",
    "\n",
    "**Ans:**\n",
    "A **Test set** is a portion of the dataset that is **not used during the model training process** but is reserved to **evaluate the model’s performance** on new, unseen data. It helps determine how well the model generalizes beyond the training data.\n",
    "\n",
    "After a model is trained on the **training set**, it is applied to the **test set** to measure metrics such as accuracy, precision, recall, F1-score, or mean squared error. These metrics indicate whether the model is overfitting, underfitting, or performing well.\n",
    "\n",
    "A common data split is:\n",
    "\n",
    "* **Training set:** 70–80%\n",
    "* **Test set:** 20–30%\n",
    "\n",
    "In Python, this can be done using:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "```\n",
    "\n",
    "The test set ensures the model is robust and reliable when deployed in real-world applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068e79d3-b2d7-47ec-bf48-599ec3282bec",
   "metadata": {},
   "source": [
    "**Q.10) How do we split data for model fitting (training and testing) in Python?**\n",
    "\n",
    "**Ans:**\n",
    "In Python, we use the `train_test_split()` function from `sklearn.model_selection` to split data into training and testing sets.\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "Here, `X` is the input data and `y` is the target. `test_size=0.2` means 20% for testing and 80% for training. `random_state` ensures reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Approaching an ML problem involves several steps:\n",
    "\n",
    "1. **Understand the problem** – Know the goal (classification, regression, etc.).\n",
    "2. **Collect and explore data** – Perform EDA to understand structure and patterns.\n",
    "3. **Preprocess the data** – Handle missing values, encode categories, scale features.\n",
    "4. **Split the data** – Use `train_test_split` to create training and testing sets.\n",
    "5. **Choose a model** – Select suitable algorithms (e.g., Linear Regression, Decision Tree).\n",
    "6. **Train the model** – Use `.fit()` on training data.\n",
    "7. **Evaluate the model** – Use the test set and metrics like accuracy or RMSE.\n",
    "8. **Tune and improve** – Apply hyperparameter tuning or feature selection.\n",
    "9. **Deploy** – Use the model in real-world applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceeb6ae-c61c-4e70-9658-965b1e441276",
   "metadata": {},
   "source": [
    "**Q.11) Why do we have to perform EDA before fitting a model to the data?**\n",
    "\n",
    "**Ans:**\n",
    "**EDA (Exploratory Data Analysis)** is a critical step in Machine Learning that helps you understand the structure, quality, and patterns in your data before applying any model. It involves summarizing main characteristics using visualizations and statistics.\n",
    "\n",
    "**Reasons to perform EDA:**\n",
    "\n",
    "1. **Identify missing or incorrect data** – EDA helps detect missing values, outliers, or inconsistencies that can harm model performance.\n",
    "2. **Understand data distributions** – Knowing how features are spread guides feature scaling and transformation.\n",
    "3. **Detect relationships** – EDA reveals correlations between variables, useful for feature selection.\n",
    "4. **Select suitable models** – Based on data types and patterns, you can choose the right ML algorithm.\n",
    "5. **Avoid garbage-in, garbage-out** – Without EDA, feeding raw or dirty data into a model can lead to poor results or misleading insights.\n",
    "\n",
    "EDA improves the quality and accuracy of your model by ensuring you work with clean, meaningful, and well-understood data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29c42b1-8e0f-4d12-b0b7-0792b00e67f4",
   "metadata": {},
   "source": [
    "**Q.12) What is correlation?**\n",
    "\n",
    "**Ans:**\n",
    "**Correlation** is a statistical measure that describes the strength and direction of the relationship between two variables. It helps in understanding whether an increase or decrease in one variable corresponds to an increase or decrease in another. Correlation values range from **-1 to +1**:\n",
    "\n",
    "* A value of **+1** indicates a perfect positive correlation (both variables increase together).\n",
    "* A value of **-1** indicates a perfect negative correlation (one increases while the other decreases).\n",
    "* A value of **0** means no correlation (variables do not affect each other).\n",
    "\n",
    "In Machine Learning, correlation is useful in feature selection, helping to identify which variables are related and which may be redundant.\n",
    "\n",
    "\n",
    "**Negative correlation** means that as one variable increases, the other variable tends to decrease. It indicates an **inverse relationship** between two variables. The correlation coefficient for a negative correlation lies between **0 and -1**.\n",
    "\n",
    "For example, if the number of hours a person exercises increases, their weight might decrease — showing a negative correlation. In Machine Learning, identifying negative correlations helps in understanding the impact of one feature on another and in selecting the most relevant variables for model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eef447e-d46e-4cf2-bbae-c6642eaf09fb",
   "metadata": {},
   "source": [
    "**Q.13) What does negative correlation mean?**\n",
    "\n",
    "**Ans:**\n",
    "**Negative correlation** means that as one variable **increases**, the other **decreases**, and vice versa. It indicates an **inverse relationship** between the two variables.\n",
    "\n",
    "The correlation coefficient for a negative correlation lies between **0 and -1**:\n",
    "\n",
    "* A value close to **-1** shows a strong negative relationship.\n",
    "* A value near **0** suggests a weak or no correlation.\n",
    "\n",
    "**Example:**\n",
    "If the number of hours spent watching TV increases, academic performance may decrease. This suggests a **negative correlation** between TV time and grades.\n",
    "\n",
    "In Machine Learning, detecting negative correlation helps in understanding how features influence the target variable and can guide **feature selection** or **interpretability** of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfb8bf3-edaf-4d5c-9f93-528784ab9f6c",
   "metadata": {},
   "source": [
    "**Q.14) How can you find correlation between variables in Python?**\n",
    "\n",
    "**Ans:**\n",
    "In Python, you can find the correlation between variables using **Pandas**. The most common method is using the `.corr()` function on a DataFrame, which calculates the **Pearson correlation coefficient** by default.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {\n",
    "    'Age': [25, 30, 35, 40],\n",
    "    'Salary': [40000, 50000, 60000, 70000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Find correlation\n",
    "correlation_matrix = df.corr()\n",
    "print(correlation_matrix)\n",
    "```\n",
    "\n",
    "This returns a matrix showing correlation values between each pair of variables.\n",
    "\n",
    "You can also visualize correlations using a **heatmap** with `seaborn`:\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This helps identify both **positive** and **negative correlations** visually.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0001fc1-5c11-44d0-bcd1-81216c6774d8",
   "metadata": {},
   "source": [
    "**Q.15) What is causation? Explain difference between correlation and causation with an example.**\n",
    "\n",
    "**Ans:**\n",
    "**Causation** means that **one variable directly affects or causes a change** in another. It shows a **cause-and-effect relationship**.\n",
    "\n",
    "**Correlation**, on the other hand, only indicates that two variables move together (positively or negatively), but it **does not imply** that one causes the other.\n",
    "\n",
    "### **Key Difference:**\n",
    "\n",
    "* **Correlation**: Relationship **without** proof of cause.\n",
    "* **Causation**: One variable **directly influences** the other.\n",
    "\n",
    "### **Example:**\n",
    "\n",
    "* **Correlation**: Ice cream sales and drowning cases both increase in summer.\n",
    "* But buying ice cream doesn’t cause drowning — the **common cause is hot weather**.\n",
    "* **Causation**: Smoking leads to lung disease — this is a **proven cause-and-effect** relationship.\n",
    "\n",
    "In Machine Learning and statistics, it's crucial to **not confuse correlation with causation**, as decisions based solely on correlations can be misleading without proper analysis or experimentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7d0f85-9c5c-45f5-96aa-0376247d66a7",
   "metadata": {},
   "source": [
    "**Q.16) What is an Optimizer? What are different types of optimizers? Explain each with an example.**\n",
    "\n",
    "**Ans:**\n",
    "An **optimizer** is an algorithm that updates a machine learning model’s **parameters (weights and biases)** to **minimize the loss function** during training. Optimizers are crucial for improving model accuracy.\n",
    "\n",
    "###  Common Types of Optimizers:\n",
    "\n",
    "1. **Gradient Descent (GD):**\n",
    "   Updates weights by calculating the gradient (slope) of the loss function.\n",
    "   *Example:* Used in basic linear regression.\n",
    "\n",
    "2. **Stochastic Gradient Descent (SGD):**\n",
    "   Updates weights using one sample at a time, making it faster but noisier.\n",
    "   *Example:* Suitable for large datasets and online learning.\n",
    "\n",
    "3. **Adam (Adaptive Moment Estimation):**\n",
    "   Combines momentum and RMSProp. It adapts the learning rate during training.\n",
    "   *Example:* Widely used in deep learning tasks.\n",
    "\n",
    "4. **RMSProp:**\n",
    "   Uses a moving average of squared gradients to adjust learning rates.\n",
    "   *Example:* Works well in recurrent neural networks.\n",
    "\n",
    "---\n",
    "\n",
    "**Q.17) What is sklearn.linear\\_model?**\n",
    "\n",
    "**Ans:**\n",
    "`sklearn.linear_model` is a module in **Scikit-learn** that provides **linear models** for regression and classification tasks.\n",
    "\n",
    "### Common models in `sklearn.linear_model`:\n",
    "\n",
    "* `LinearRegression`: For predicting continuous values.\n",
    "* `LogisticRegression`: For binary or multiclass classification.\n",
    "* `Ridge`, `Lasso`: Regularized versions of linear regression to prevent overfitting.\n",
    "\n",
    "These models are simple, fast, and often serve as a good baseline for ML problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20400217-ec6c-4640-a085-fa7b5ee706ac",
   "metadata": {},
   "source": [
    "**Q.18) What does model.fit() do? What arguments must be given?**\n",
    "\n",
    "**Ans:**\n",
    "`model.fit()` is used to **train** a Machine Learning model. It learns the relationship between input features (**X**) and the target/output (**y**) by adjusting internal parameters.\n",
    "\n",
    "### Required Arguments:\n",
    "\n",
    "* `X`: Input data (features) – usually a 2D array or DataFrame.\n",
    "* `y`: Target labels (output values).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Q.19) What does model.predict() do? What arguments must be given?**\n",
    "\n",
    "**Ans:**\n",
    "`model.predict()` uses the **trained model** to make predictions on new or unseen input data.\n",
    "\n",
    "### Required Argument:\n",
    "\n",
    "* `X`: Input data for which predictions are to be made (same format as used in training).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Q.20) What are continuous and categorical variables?**\n",
    "\n",
    "**Ans:**\n",
    "\n",
    "* **Continuous Variables:**\n",
    "  These are **numeric** and can take any value within a range.\n",
    "  *Example:* Height, weight, age, temperature.\n",
    "\n",
    "* **Categorical Variables:**\n",
    "  These represent **categories or labels**. They may be text or numbers but denote class or group.\n",
    "  *Example:* Gender (Male/Female), Color (Red/Blue), City names.\n",
    "\n",
    "They require **different preprocessing techniques** in Machine Learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a951b70-13fb-43a2-9c60-d323a402b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Q.24) What is feature scaling? How does it help in Machine Learning?**\n",
    "\n",
    "**Ans:**\n",
    "**Feature scaling** is the process of normalizing or standardizing the range of independent variables (features). It ensures that all features contribute **equally** to the result and prevents one from dominating due to scale.\n",
    "\n",
    "It improves model performance, especially in algorithms like KNN, SVM, and gradient descent-based models.\n",
    "\n",
    "---\n",
    "\n",
    "**Q.25) How do we perform scaling in Python?**\n",
    "\n",
    "**Ans:**\n",
    "We use **`sklearn.preprocessing`** tools like:\n",
    "\n",
    "* `StandardScaler()` – standardizes data (mean = 0, std = 1)\n",
    "* `MinMaxScaler()` – scales data to a specific range (usually 0 to 1)\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "scaler = StandardScaler()  \n",
    "X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Q.2) What is sklearn.preprocessing?**\n",
    "\n",
    "**Ans:**\n",
    "`sklearn.preprocessing` is a module in Scikit-learn that provides tools for **data transformation**, including:\n",
    "\n",
    "* Feature scaling (StandardScaler, MinMaxScaler)\n",
    "* Encoding (LabelEncoder, OneHotEncoder)\n",
    "* Handling missing values (SimpleImputer)\n",
    "\n",
    "---\n",
    "\n",
    "**Q.26) How do we split data for model fitting (training and testing) in Python?**\n",
    "\n",
    "**Ans:**\n",
    "Use `train_test_split()` from `sklearn.model_selection`:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Q.27) Explain data encoding.**\n",
    "\n",
    "**Ans:**\n",
    "**Data encoding** converts **categorical values into numerical format**, allowing ML models to interpret them.\n",
    "\n",
    "Common methods:\n",
    "\n",
    "* **Label Encoding:** Assigns an integer to each category.\n",
    "* **One-Hot Encoding:** Creates binary columns for each category.\n",
    "\n",
    "Tools: `LabelEncoder`, `OneHotEncoder`, or `pandas.get_dummies()`\n",
    "\n",
    "Encoding is essential for training models on non-numeric data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
